{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import spacy\n",
    "import collections\n",
    "import operator\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Loading the [Sentiment140](https://www.kaggle.com/kazanova/sentiment140) Dataset that includes 1.6 milion tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "encoding = \"ISO-8859-1\"\n",
    "dataset = pd.read_csv('dataset/tweets.csv', encoding=encoding, names=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns with not useful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.drop(columns=[\"ids\", \"date\", \"flag\", \"user\"], inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original dataset is using classes `0-Negative` and `4-Positive`, I am mapping the `4` to `1` to fix the confusing naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset.target.replace({4: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets shuffle the rows in dataset, since originally they are sorted by the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "# Remove for final training\n",
    "dataset = dataset.iloc[:120_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Dataset is now ready for preprocessing. I will use [spacy](https://spacy.io) which is a very popular library for Natural Language Processing.\n",
    "\n",
    "Firstly, lets load a model that will help us with tokenisation and lemmatisation of tweets included in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piping through the tweets from dataset, will result in a iterator object that we will need to use to iterate over preprocessed tweets - and tokenized words in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_iterator = nlp.pipe(dataset.text, n_threads=-1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out which are the most popular words in the dataset by mapping them to a dictionary. This can be useful during the optimalisation process, if we would want to drop some rare words to not include them in the final model to reduce its size. This process can take quite a while, since we are operating on a very large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_to_lexeme = {}\n",
    "words = collections.defaultdict(int)\n",
    "preprocessed_text = []\n",
    "for tweet in tqdm(tweets_iterator, total=dataset.shape[0]):\n",
    "    preprocessed_tweet = []\n",
    "    for token in tweet:\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        lexeme = nlp.vocab[token.lemma]\n",
    "        if lexeme.has_vector:\n",
    "            words_to_lexeme[lexeme.text.lower()] = lexeme\n",
    "            words[lexeme.text.lower()] += 1\n",
    "            preprocessed_tweet.append(lexeme.text)\n",
    "    preprocessed_text.append(preprocessed_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets update our dataset with new column that will contain preprocessed tweet contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['preprocessed'] = preprocessed_text\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets drop words that occur only once to reduce the model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of words before reducing size:', len(words))\n",
    "# keys = list(words.keys())\n",
    "# for key in keys:\n",
    "#     if words[key] < 2:\n",
    "#         del words[key]\n",
    "# print('Number of words before reducing size:', len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting the counts, with its corresponding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_words = sorted(words.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print('Top 10 words are:')\n",
    "_ = [print(word, count) for word, count in sorted_words[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once we have the tokens, sorted by their popularity, we will need to create the `embedding_matrix`. The matrix consists of vectors for each token/word that we get from spacy. One important thing to notice is that we will make an extra entry in the matrix at index 0, that will represent the placeholder for empty word. To train the model (and to recieve the prediction) we will feed it with array of ids of words from the tweet. These array will need to be equal lenght, because neural network will always expect the input to be equal size. We will use this row `0` to refer to a empty token, so we can pad the input array to match the expected by neural network shape of the input. \n",
    "\n",
    "Also I will create a `word_index` array to keep the reference of word to index so we can convert that later for proper neural network input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {'': 0}\n",
    "top_embedding_matrix = np.zeros((len(words) + 1, 300))\n",
    "\n",
    "for i, word in enumerate(words, start=1):\n",
    "    word_index[word[0]] = i\n",
    "    embedding_matrix[i] = words_to_lexeme[word[0]].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting matrix is a very important part of the model, since it will serve as a weights matrix in the first layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "At this point, once we are ready with the preprocessing data, lets design the architecture for our model. I will use [Keras](https://keras.io), which is a very popular Deep Learning library.\n",
    "\n",
    "The model is a very simple Convolutional Neural Network consisting of 9 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(len(top_embedding_matrix), 300, weights=[top_embedding_matrix], input_length=100),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "This is the part where we jump to the training our model. Lets split out data into training and testing subsets of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset.preprocessed, dataset.target, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we will pipe our data into the model, there is one last thing to do. We need to map the words form tweet to ids, and to fill the array with zeros so the model will accept it as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_words(row):\n",
    "    return np.array([word_index[word.lower()] if word.lower() in word_index else 0 for word in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(map_words)\n",
    "X_test = X_test.apply(map_words)\n",
    "X_train = pad_sequences(X_train, maxlen=100)\n",
    "X_test = pad_sequences(X_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train data shapes:', 'X', X_train.shape, 'y', y_train.shape)\n",
    "print('Test data shapes:', 'X', X_test.shape, 'y', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, finally! Let's go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the model on our own tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed(tweets):\n",
    "    preprocessed = []\n",
    "    for tweet in nlp.pipe(tweets, n_threads=-1, batch_size=32):\n",
    "        preprocessed_tweet = []\n",
    "        for token in tweet:\n",
    "            if token.is_stop:\n",
    "                continue\n",
    "            lexeme = nlp.vocab[token.lemma]\n",
    "            if lexeme.has_vector:\n",
    "                preprocessed_tweet.append(lexeme.text)\n",
    "        preprocessed.append(preprocessed_tweet)\n",
    "    print(preprocessed)\n",
    "    preprocessed = [map_words(tweet) for tweet in preprocessed]\n",
    "    return pad_sequences(preprocessed, maxlen=100)[0].reshape(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    'Devastated to hear about the lives lost in Lee County, a place close to my heart. Wishing strength and healing for all those affected by today’s tornadoes.',\n",
    "    'Impressive work by these young photographers! Using iPhone XR, more than 260 schoolchildren from around Chennai, India captured images of their community for the annual #ChennaiPhotoBiennale. #ShotoniPhone',\n",
    "    'Saddened to hear of Pierre Nanterme’s passing. He will be remembered for his contributions to business as well as his work toward equality in the workplace. Our deepest condolences go out to his friends, family and everyone at Accenture.',\n",
    "    'Steve’s vision is reflected all around us at Apple Park. He would have loved it here, in this place he dreamed up — the home and inspiration for Apple’s future innovations. We miss him today on his 64th birthday, and every day.'\n",
    "]\n",
    "\n",
    "for tweet in tweets:\n",
    "    preped = preprocessed([tweet])\n",
    "    print('Tweet :', tweet)\n",
    "    print('Sentiment :', model2.predict(preped)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export\n",
    "\n",
    "After we are done with training and our model is ready, we have to export the model to the file so I can convert it to `.mlmodel` with `coremltools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('SentimentalAnalysisModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the word index needs to be exported and a simple txt file will be just enough. The line number will serve as id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words.txt\", \"w\") as text_file:\n",
    "    for key in word_index.keys():\n",
    "        text_file.write(key + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
