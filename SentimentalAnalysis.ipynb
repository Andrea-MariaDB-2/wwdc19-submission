{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import spacy\n",
    "import collections\n",
    "import operator\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Loading the [Sentiment140](https://www.kaggle.com/kazanova/sentiment140) Dataset that includes 1.6 milion tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.16 s, sys: 339 ms, total: 3.5 s\n",
      "Wall time: 4.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "encoding = \"ISO-8859-1\"\n",
    "dataset = pd.read_csv('dataset/tweets.csv', encoding=encoding, names=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns with not useful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0  @Kenichan I dived many times for the ball. Man...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop(columns=[\"ids\", \"date\", \"flag\", \"user\"], inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original dataset is using classes `0-Negative` and `4-Positive`, I am mapping the `4` to `1` to fix the confusing naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset.target.replace({4: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets shuffle the rows in dataset, since originally they are sorted by the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "# Remove for final training\n",
    "dataset = dataset.iloc[:120_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@darkdaisy Yah. I actually am going. I booked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>School's out! Let the mayhem commence! Ideally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Now Jump Up Let's Get Krazyyy!  In such a part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@LMsouthgate laaura do u know? moises arias ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I wish I was still sleeping</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       1  @darkdaisy Yah. I actually am going. I booked ...\n",
       "1       1  School's out! Let the mayhem commence! Ideally...\n",
       "2       0  Now Jump Up Let's Get Krazyyy!  In such a part...\n",
       "3       1  @LMsouthgate laaura do u know? moises arias ta...\n",
       "4       0                       I wish I was still sleeping "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Dataset is now ready for preprocessing. I will use [spacy](https://spacy.io) which is a very popular library for Natural Language Processing.\n",
    "\n",
    "Firstly, lets load a model that will help us with tokenisation and lemmatisation of tweets included in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.87 s, sys: 1.44 s, total: 8.3 s\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piping through the tweets from dataset, will result in a iterator object that we will need to use to iterate over preprocessed tweets - and tokenized words in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_iterator = nlp.pipe(dataset.text, n_threads=-1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out which are the most popular words in the dataset by mapping them to a dictionary. This can be useful during the optimalisation process, if we would want to drop some rare words to not include them in the final model to reduce its size. This process can take quite a while, since we are operating on a very large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1571a0b11cf4c398d66e059d5454c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=120000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "words_to_lexeme = {}\n",
    "words = collections.defaultdict(int)\n",
    "preprocessed_text = []\n",
    "for tweet in tqdm(tweets_iterator, total=dataset.shape[0]):\n",
    "    preprocessed_tweet = []\n",
    "    for token in tweet:\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        lexeme = nlp.vocab[token.lemma]\n",
    "        if lexeme.has_vector:\n",
    "            words_to_lexeme[lexeme.text.lower()] = lexeme\n",
    "            words[lexeme.text.lower()] += 1\n",
    "            preprocessed_tweet.append(lexeme.text)\n",
    "    preprocessed_text.append(preprocessed_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets update our dataset with new column that will contain preprocessed tweet contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@darkdaisy Yah. I actually am going. I booked ...</td>\n",
       "      <td>[yah, ., actually, go, ., book, ., Alaska, ., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>School's out! Let the mayhem commence! Ideally...</td>\n",
       "      <td>[school, !, let, mayhem, commence, !, ideally,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Now Jump Up Let's Get Krazyyy!  In such a part...</td>\n",
       "      <td>[jump, let, !, party, mood, ., bad, ground, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@LMsouthgate laaura do u know? moises arias ta...</td>\n",
       "      <td>[u, know, ?, moise, aria, talk, mee, !, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I wish I was still sleeping</td>\n",
       "      <td>[wish, sleep]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  \\\n",
       "0       1  @darkdaisy Yah. I actually am going. I booked ...   \n",
       "1       1  School's out! Let the mayhem commence! Ideally...   \n",
       "2       0  Now Jump Up Let's Get Krazyyy!  In such a part...   \n",
       "3       1  @LMsouthgate laaura do u know? moises arias ta...   \n",
       "4       0                       I wish I was still sleeping    \n",
       "\n",
       "                                        preprocessed  \n",
       "0  [yah, ., actually, go, ., book, ., Alaska, ., ...  \n",
       "1  [school, !, let, mayhem, commence, !, ideally,...  \n",
       "2     [jump, let, !, party, mood, ., bad, ground, .]  \n",
       "3         [u, know, ?, moise, aria, talk, mee, !, !]  \n",
       "4                                      [wish, sleep]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['preprocessed'] = preprocessed_text\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting the counts, with its corresponding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words are:\n",
      "! 68007\n",
      ". 60375\n",
      ", 36121\n",
      "? 17951\n",
      "... 16535\n",
      "- 8877\n",
      "go 8753\n",
      "good 8160\n",
      "day 7935\n",
      "get 7315\n"
     ]
    }
   ],
   "source": [
    "sorted_words = sorted(words.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print('Top 10 words are:')\n",
    "_ = [print(word, count) for word, count in sorted_words[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once we have the tokens, sorted by their popularity, we will need to create the `embedding_matrix`. The matrix consists of vectors for each token/word that we get from spacy. One important thing to notice is that we will make an extra entry in the matrix at index 0, that will represent the placeholder for empty word. To train the model (and to recieve the prediction) we will feed it with array of ids of words from the tweet. These array will need to be equal lenght, because neural network will always expect the input to be equal size. We will use this row `0` to refer to a empty token, so we can pad the input array to match the expected by neural network shape of the input. \n",
    "\n",
    "Also I will create a `word_index` array to keep the reference of word to index so we can convert that later for proper neural network input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {'': 0}\n",
    "embedding_matrix = np.zeros((len(words) + 1, 300))\n",
    "\n",
    "for i, word in enumerate(words, start=1):\n",
    "    word_index[word] = i\n",
    "    embedding_matrix[i] = words_to_lexeme[word].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting matrix is a very important part of the model, since it will serve as a weights matrix in the first layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.32356   , -0.097893  , -0.33511999, ...,  0.2814    ,\n",
       "        -0.30553001, -0.089444  ],\n",
       "       [ 0.012001  ,  0.20750999, -0.12578   , ...,  0.13871001,\n",
       "        -0.36048999, -0.035     ],\n",
       "       ...,\n",
       "       [ 0.053052  ,  0.13817   , -0.51157999, ...,  0.39816001,\n",
       "         0.21936999, -0.094792  ],\n",
       "       [ 0.34850001, -0.56590998, -0.1804    , ...,  0.79347003,\n",
       "         0.33541   , -0.096093  ],\n",
       "       [ 0.0069292 , -0.64995998, -0.43669   , ..., -0.22797   ,\n",
       "         0.30250001, -0.23746   ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "At this point, once we are ready with the preprocessing data, lets design the architecture for our model. I will use [Keras](https://keras.io), which is a very popular Deep Learning library.\n",
    "\n",
    "The model is a very simple Convolutional Neural Network consisting of 9 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/witekbobrowski/Library/Python/3.7/lib/python/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          10971300  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 11,131,801\n",
      "Trainable params: 11,131,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(len(top_embedding_matrix), 300, weights=[top_embedding_matrix], input_length=100),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "This is the part where we jump to the training our model. Lets split out data into training and testing subsets of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset.preprocessed, dataset.target, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we will pipe our data into the model, there is one last thing to do. We need to map the words form tweet to ids, and to fill the array with zeros so the model will accept it as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_words(row):\n",
    "    return np.array([word_index[word.lower()] if word.lower() in word_index else 0 for word in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(map_words)\n",
    "X_test = X_test.apply(map_words)\n",
    "X_train = pad_sequences(X_train, maxlen=100)\n",
    "X_test = pad_sequences(X_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shapes: X (96000, 100) y (96000,)\n",
      "Test data shapes: X (24000, 100) y (24000,)\n"
     ]
    }
   ],
   "source": [
    "print('Train data shapes:', 'X', X_train.shape, 'y', y_train.shape)\n",
    "print('Test data shapes:', 'X', X_test.shape, 'y', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, finally! Let's go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 86400 samples, validate on 9600 samples\n",
      "Epoch 1/5\n",
      "86400/86400 [==============================] - 423s 5ms/step - loss: 0.5250 - acc: 0.7343 - val_loss: 0.5005 - val_acc: 0.7551\n",
      "Epoch 2/5\n",
      "86400/86400 [==============================] - 408s 5ms/step - loss: 0.4496 - acc: 0.7895 - val_loss: 0.5046 - val_acc: 0.7511\n",
      "Epoch 3/5\n",
      "86400/86400 [==============================] - 401s 5ms/step - loss: 0.4016 - acc: 0.8160 - val_loss: 0.5200 - val_acc: 0.7479\n",
      "Epoch 4/5\n",
      "86400/86400 [==============================] - 448s 5ms/step - loss: 0.3567 - acc: 0.8393 - val_loss: 0.5593 - val_acc: 0.7427\n",
      "Epoch 5/5\n",
      "86400/86400 [==============================] - 481s 6ms/step - loss: 0.3153 - acc: 0.8569 - val_loss: 0.6116 - val_acc: 0.7332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x174184b00>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/24000 [==============================] - 16s 650us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6055881338914235, 0.73875]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the model on some custom tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed(tweets):\n",
    "    preprocessed = []\n",
    "    for tweet in nlp.pipe(tweets, n_threads=-1, batch_size=32):\n",
    "        preprocessed_tweet = []\n",
    "        for token in tweet:\n",
    "            if token.is_stop:\n",
    "                continue\n",
    "            lexeme = nlp.vocab[token.lemma]\n",
    "            if lexeme.has_vector:\n",
    "                preprocessed_tweet.append(lexeme.text)\n",
    "        preprocessed.append(preprocessed_tweet)\n",
    "    preprocessed = [map_words(tweet) for tweet in preprocessed]\n",
    "    return pad_sequences(preprocessed, maxlen=100)[0].reshape(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet : Devastated to hear about the lives lost in Lee County, a place close to my heart. Wishing strength and healing for all those affected by today’s tornadoes.\n",
      "Sentiment : 0.0011292003 \n",
      "\n",
      "Tweet : Impressive work by these young photographers! Using iPhone XR, more than 260 schoolchildren from around Chennai, India captured images of their community for the annual #ChennaiPhotoBiennale. #ShotoniPhone\n",
      "Sentiment : 0.045893963 \n",
      "\n",
      "Tweet : Saddened to hear of Pierre Nanterme’s passing. He will be remembered for his contributions to business as well as his work toward equality in the workplace. Our deepest condolences go out to his friends, family and everyone at Accenture.\n",
      "Sentiment : 0.04379572 \n",
      "\n",
      "Tweet : Steve’s vision is reflected all around us at Apple Park. He would have loved it here, in this place he dreamed up — the home and inspiration for Apple’s future innovations. We miss him today on his 64th birthday, and every day.\n",
      "Sentiment : 0.2523569 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = [\n",
    "    'Devastated to hear about the lives lost in Lee County, a place close to my heart. Wishing strength and healing for all those affected by today’s tornadoes.',\n",
    "    'Impressive work by these young photographers! Using iPhone XR, more than 260 schoolchildren from around Chennai, India captured images of their community for the annual #ChennaiPhotoBiennale. #ShotoniPhone',\n",
    "    'Saddened to hear of Pierre Nanterme’s passing. He will be remembered for his contributions to business as well as his work toward equality in the workplace. Our deepest condolences go out to his friends, family and everyone at Accenture.',\n",
    "    'Steve’s vision is reflected all around us at Apple Park. He would have loved it here, in this place he dreamed up — the home and inspiration for Apple’s future innovations. We miss him today on his 64th birthday, and every day.'\n",
    "]\n",
    "\n",
    "for tweet in tweets:\n",
    "    preped = preprocessed([tweet])\n",
    "    print('Tweet :', tweet)\n",
    "    print('Sentiment :', model.predict(preped)[0][0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export\n",
    "\n",
    "After we are done with training and our model is ready, we have to export the model to the file so I can convert it to `.mlmodel` with `coremltools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SentimentalAnalysisModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the word index needs to be exported and a simple txt file will be just enough. The line number will serve as id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words.txt\", \"w\") as text_file:\n",
    "    for key in word_index.keys():\n",
    "        text_file.write(key + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
